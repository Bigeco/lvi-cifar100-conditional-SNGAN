{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6167c57f-9c36-48c5-b633-7bccee3703ce",
   "metadata": {},
   "source": [
    "# 0. Import & Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea086cb-e8c7-4502-ac9e-cc3f9d27d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준 라이브러리\n",
    "import argparse\n",
    "import datetime\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# 서드파티 라이브러리\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch 관련 라이브러리\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from torch.nn import init, utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter as P, init, utils\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 비활성화\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5826bf-5278-4e1f-937a-874f46242498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training information\n",
    "TRAIN = True\n",
    "VERSION = 'cifar100_superclass'\n",
    "\n",
    "# Model hyper-parameters\n",
    "IMSIZE = 32\n",
    "Z_DIM = 128\n",
    "G_CONV_DIM = 128\n",
    "D_CONV_DIM = 128\n",
    "N_CLASS = 20\n",
    "GEN_DISTRIBUTION = 'normal' \n",
    "GEN_BOTTOM_WIDTH = 4  \n",
    "SEED = 46 \n",
    "FIX_SEED = False # If you want to fix the seed, set this to True.\n",
    "\n",
    "# Training setting\n",
    "TOTAL_STEP = 1000000\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "G_LR = 0.0002\n",
    "D_LR = 0.0002\n",
    "LR_DECAY = 0.95\n",
    "BETA1 = 0.0\n",
    "BETA2 = 0.999\n",
    "D_STEP = 3\n",
    "G_STEP = 1\n",
    "\n",
    "# Misc\n",
    "CUDA = 0\n",
    "\n",
    "# Path\n",
    "IMAGE_PATH = './data'\n",
    "LOG_PATH = './logs'\n",
    "MODEL_SAVE_PATH = './models'\n",
    "SAMPLE_PATH = './samples'\n",
    "FID_MEAN_COV = './datasetMoment/cifar100'\n",
    "\n",
    "# Step size\n",
    "LOG_STEP = 50\n",
    "SAMPLE_STEP = 500\n",
    "MODEL_SAVE_STEP = 5000\n",
    "METRIC_CALCULATION_STEP = 2000\n",
    "CALCULATE_FID = True\n",
    "METRIC_IMAGES_NUM = 200\n",
    "\n",
    "# Checkpoint\n",
    "RESUME_TRAINING = False \n",
    "RESUME_CHECKPOINT = None\n",
    "\n",
    "# ====================== Example ====================== #\n",
    "# RESUME_TRAINING = True \n",
    "# RESUME_CHECKPOINT = 'models/cifar100_superclass/checkpoints/checkpoint_130000.pt'\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if FIX_SEED:\n",
    "    seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db610c7-1622-4246-9b56-0e3c0b96a315",
   "metadata": {},
   "source": [
    "# 1. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08efab3a-ca90-4996-ac09-2363547078d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporter\n",
    "class Reporter:\n",
    "    def __init__(self, reportPath):\n",
    "        self.path = reportPath\n",
    "        self.withTimeStamp = False\n",
    "        self.index = 1\n",
    "        self.timeStrFormat = '%Y-%m-%d %H:%M:%S'\n",
    "        \n",
    "        # KST 시간대 설정\n",
    "        self.kst = datetime.timezone(datetime.timedelta(hours=9))\n",
    "        \n",
    "        # 파일명에 한국 시간 타임스탬프 추가\n",
    "        now = datetime.datetime.now(self.kst)\n",
    "        timeStr = now.strftime('%Y%m%d%H%M%S')\n",
    "        self.path = self.path + \".%s\" % timeStr\n",
    "        \n",
    "        # 파일이 없으면 생성\n",
    "        if not os.path.exists(self.path):\n",
    "            f = open(self.path, 'w')\n",
    "            f.close()\n",
    "\n",
    "    def writeInfo(self, strLine):\n",
    "        with open(self.path, 'a+') as logf:\n",
    "            # 현재 한국 시간 가져오기\n",
    "            now = datetime.datetime.now(self.kst)\n",
    "            timeStr = now.strftime(self.timeStrFormat)\n",
    "            logf.writelines(\"[%d]-[%s]-[info] %s\\n\" % (self.index, timeStr, strLine))\n",
    "            self.index += 1\n",
    "\n",
    "    def writeModel(self, modelText):\n",
    "        with open(self.path, 'a+') as logf:\n",
    "            logf.writelines(\"[%d]-[model] %s\\n\" % (self.index, modelText))\n",
    "            self.index += 1\n",
    "\n",
    "    def writeTrainLog(self, step, logText):\n",
    "        with open(self.path, 'a+') as logf:\n",
    "            # 현재 한국 시간 가져오기\n",
    "            now = datetime.datetime.now(self.kst)\n",
    "            timeStr = now.strftime(self.timeStrFormat)\n",
    "            logf.writelines(\"[%d]-[%s]-[logInfo]-[%d] %s\\n\" % (self.index, timeStr, step, logText))\n",
    "            self.index += 1\n",
    "\n",
    "def denorm(x):\n",
    "    \"\"\"Convert the range from [-1, 1] to [0, 1]\"\"\"\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp_(0, 1)\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "    \n",
    "def makeFolder(path, version):\n",
    "    if not os.path.exists(os.path.join(path, version)):\n",
    "        os.makedirs(os.path.join(path, version))\n",
    "\n",
    "# Decorator\n",
    "def time_it(fn):\n",
    "    def new_fn(*args):\n",
    "        start = time.time()\n",
    "        result = fn(*args)\n",
    "        end = time.time()\n",
    "        duration = end - start\n",
    "        print('%.4f seconds are consumed in executing function:%s'\\\n",
    "              %(duration, fn.__name__))\n",
    "        return result\n",
    "    return new_fn\n",
    "\n",
    "def load_inception_net():\n",
    "    \"\"\"Pre-trained inception network 로드\"\"\"\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False)\n",
    "    inception_model = WrapInception(inception_model.eval()).cuda()\n",
    "    return inception_model\n",
    "\n",
    "class WrapInception(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super(WrapInception,self).__init__()\n",
    "        self.net = net\n",
    "        self.mean = P(torch.tensor([0.485, 0.456, 0.406]).view(1, -1, 1, 1),\n",
    "                    requires_grad=False)\n",
    "        self.std = P(torch.tensor([0.229, 0.224, 0.225]).view(1, -1, 1, 1),\n",
    "                   requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize x\n",
    "        x = (x + 1.) / 2.0\n",
    "        x = (x - self.mean) / self.std\n",
    "        # Upsample if necessary\n",
    "        if x.shape[2] != 299 or x.shape[3] != 299:\n",
    "            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=True)\n",
    "        # Get inception features\n",
    "        x = self.net.Conv2d_1a_3x3(x)\n",
    "        x = self.net.Conv2d_2a_3x3(x)\n",
    "        x = self.net.Conv2d_2b_3x3(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        x = self.net.Conv2d_3b_1x1(x)\n",
    "        x = self.net.Conv2d_4a_3x3(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        x = self.net.Mixed_5b(x)\n",
    "        x = self.net.Mixed_5c(x)\n",
    "        x = self.net.Mixed_5d(x)\n",
    "        x = self.net.Mixed_6a(x)\n",
    "        x = self.net.Mixed_6b(x)\n",
    "        x = self.net.Mixed_6c(x)\n",
    "        x = self.net.Mixed_6d(x)\n",
    "        x = self.net.Mixed_6e(x)\n",
    "        x = self.net.Mixed_7a(x)\n",
    "        x = self.net.Mixed_7b(x)\n",
    "        x = self.net.Mixed_7c(x)\n",
    "        pool = torch.mean(x.view(x.size(0), x.size(1), -1), 2)\n",
    "        logits = self.net.fc(F.dropout(pool, training=False).view(pool.size(0), -1))\n",
    "        return pool, logits\n",
    "\n",
    "# Sampler\n",
    "class Distribution(torch.Tensor):\n",
    "    # Init the params of the distribution\n",
    "    def init_distribution(self, dist_type, **kwargs):\n",
    "        self.dist_type = dist_type\n",
    "        self.dist_kwargs = kwargs\n",
    "        if self.dist_type == 'normal':\n",
    "            self.mean, self.var = kwargs['mean'], kwargs['var']\n",
    "        elif self.dist_type == 'categorical':\n",
    "            self.num_categories = kwargs['num_categories']\n",
    "\n",
    "    def sample_(self):\n",
    "        if self.dist_type == 'normal':\n",
    "            self.normal_(self.mean, self.var)\n",
    "        elif self.dist_type == 'categorical':\n",
    "            self.random_(0, self.num_categories)\n",
    "    # return self.variable\n",
    "\n",
    "    # Silly hack: overwrite the to() method to wrap the new object\n",
    "    # in a distribution as well\n",
    "    def to(self, *args, **kwargs):\n",
    "        new_obj = Distribution(self)\n",
    "        new_obj.init_distribution(self.dist_type, **self.dist_kwargs)\n",
    "        new_obj.data = super().to(*args, **kwargs)\n",
    "        return new_obj\n",
    "\n",
    "# Sampling functions\n",
    "def prepare_z_c(G_batch_size, dim_z, nclasses, device='cuda', z_var=1.0):\n",
    "    z_ = Distribution(torch.randn(G_batch_size, dim_z, requires_grad=False))\n",
    "    z_.init_distribution('normal', mean=0, var=z_var)\n",
    "    z_ = z_.to(device, torch.float32)\n",
    "    c_ = Distribution(torch.zeros(G_batch_size, requires_grad=False))\n",
    "    c_.init_distribution('categorical',num_categories=nclasses)\n",
    "    c_ = c_.to(device, torch.int64)\n",
    "    return z_,c_\n",
    "\n",
    "def prepareSampleZ(G_batch_size, dim_z, device='cuda', z_var=1.0):\n",
    "    z_ = Distribution(torch.randn(G_batch_size, dim_z, requires_grad=False))\n",
    "    z_.init_distribution('normal', mean=0, var=z_var)\n",
    "    z_ = z_.to(device, torch.float32)\n",
    "    return z_\n",
    "\n",
    "def sampleG(G, z_, c_, parallel=False):\n",
    "    with torch.no_grad():\n",
    "        z_.sample_()\n",
    "        c_.sample_()\n",
    "        if parallel:\n",
    "            G_z =  nn.parallel.data_parallel(G,[z_,c_])\n",
    "        else:\n",
    "            G_z = G(z_,c_)\n",
    "        return G_z\n",
    "\n",
    "def sampleFixedLabels(numClasses,batchSize,device):\n",
    "    a = [1]*batchSize\n",
    "    res = []\n",
    "    for i in range(numClasses):\n",
    "        res += [t*i for t in a]\n",
    "    pseudo_labels = torch.tensor(res).long().to(device)\n",
    "    return pseudo_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d19f3-4927-46df-84be-c1be920eeb9a",
   "metadata": {},
   "source": [
    "# 2. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f09521a-aa45-4023-83d4-7d82a5fd6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar100_superclass_loader(image_path, image_size, batch_size, num_workers=2):\n",
    "    \"\"\"\n",
    "    Creates a data loader for CIFAR-100 that returns superclass labels instead of fine labels\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR100(root=image_path, train=True, download=True, transform=transform)\n",
    "\n",
    "    # CIFAR-100 superclass labels (coarse_labels)\n",
    "    superclass_map = {\n",
    "        'aquatic mammals': [4, 30, 55, 72, 95],\n",
    "        'fish': [1, 32, 67, 73, 91],\n",
    "        'flowers': [54, 62, 70, 82, 92],\n",
    "        'food containers': [9, 10, 16, 28, 61],\n",
    "        'fruit and vegetables': [0, 51, 53, 57, 83],\n",
    "        'household electrical devices': [22, 39, 40, 86, 87],\n",
    "        'household furniture': [5, 20, 25, 84, 94],\n",
    "        'insects': [6, 7, 14, 18, 24],\n",
    "        'large carnivores': [3, 42, 43, 88, 97],\n",
    "        'large man-made outdoor things': [12, 17, 37, 68, 76],\n",
    "        'large natural outdoor scenes': [23, 33, 49, 60, 71],\n",
    "        'large omnivores and herbivores': [15, 19, 21, 31, 38],\n",
    "        'medium-sized mammals': [34, 35, 46, 98, 99],\n",
    "        'non-insect invertebrates': [26, 45, 77, 79, 93],\n",
    "        'people': [2, 11, 36, 66, 96],\n",
    "        'reptiles': [27, 29, 44, 78, 80],\n",
    "        'small mammals': [8, 13, 48, 58, 90],\n",
    "        'trees': [41, 47, 52, 56, 59],\n",
    "        'vehicles 1': [50, 63, 64, 65, 85],\n",
    "        'vehicles 2': [69, 74, 75, 81, 89]\n",
    "    }\n",
    "\n",
    "    # Create reverse mapping from fine label to superclass index\n",
    "    fine_to_super = {}\n",
    "    for super_idx, (_, fine_labels) in enumerate(superclass_map.items()):\n",
    "        for fine_label in fine_labels:\n",
    "            fine_to_super[fine_label] = super_idx\n",
    "\n",
    "    # Modify the dataset's targets to use superclass labels\n",
    "    dataset.targets = [fine_to_super[target] for target in dataset.targets]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ffc7c-87c0-464a-bab7-96be5698fcaa",
   "metadata": {},
   "source": [
    "# 3. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4721469-5c79-48be-8056-489139315e5b",
   "metadata": {},
   "source": [
    "## 3.1. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35534c63-28a6-41e0-9153-297dc1992913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalBatchNorm2d(nn.BatchNorm2d):\n",
    "    \"\"\"Conditional Batch Normalization\"\"\"\n",
    "\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1,\n",
    "                 affine=False, track_running_stats=True):\n",
    "        super(ConditionalBatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, affine, track_running_stats\n",
    "        )\n",
    "\n",
    "    def forward(self, input, weight, bias, **kwargs):\n",
    "        self._check_input_dim(input)\n",
    "\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            self.num_batches_tracked += 1\n",
    "            if self.momentum is None:  # use cumulative moving average\n",
    "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
    "            else:  # use exponential moving average\n",
    "                exponential_average_factor = self.momentum\n",
    "\n",
    "        output = F.batch_norm(input, self.running_mean, self.running_var,\n",
    "                              self.weight, self.bias,\n",
    "                              self.training or not self.track_running_stats,\n",
    "                              exponential_average_factor, self.eps)\n",
    "        if weight.dim() == 1:\n",
    "            weight = weight.unsqueeze(0)\n",
    "        if bias.dim() == 1:\n",
    "            bias = bias.unsqueeze(0)\n",
    "        size = output.size()\n",
    "        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
    "        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
    "        return weight * output + bias\n",
    "\n",
    "class CategoricalConditionalBatchNorm2d(ConditionalBatchNorm2d):\n",
    "    def __init__(self, num_classes, num_features, eps=1e-5, momentum=0.1,\n",
    "                 affine=False, track_running_stats=True):\n",
    "        super(CategoricalConditionalBatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, affine, track_running_stats\n",
    "        )\n",
    "        self.weights = nn.Embedding(num_classes, num_features)\n",
    "        self.biases = nn.Embedding(num_classes, num_features)\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        init.ones_(self.weights.weight.data)\n",
    "        init.zeros_(self.biases.weight.data)\n",
    "\n",
    "    def forward(self, input, c, **kwargs):\n",
    "        weight = self.weights(c)\n",
    "        bias = self.biases(c)\n",
    "\n",
    "        return super(CategoricalConditionalBatchNorm2d, self).forward(input, weight, bias)\n",
    "\n",
    "def _upsample(x):\n",
    "    h, w = x.size()[2:]\n",
    "    return F.interpolate(x, size=(h * 2, w * 2))\n",
    "\n",
    "class GenResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, h_ch=None, ksize=3, pad=1,\n",
    "                 activation=F.relu, upsample=False, num_classes=0):\n",
    "        super(GenResBlock, self).__init__()\n",
    "\n",
    "        self.activation     = activation\n",
    "        self.upsample       = upsample\n",
    "        self.learnable_sc   = in_ch != out_ch or upsample\n",
    "        if h_ch is None:\n",
    "            h_ch = out_ch\n",
    "        self.num_classes = num_classes\n",
    "        # Register layrs\n",
    "        self.c1 = nn.Conv2d(in_ch, h_ch, ksize, 1, pad)\n",
    "        self.c2 = nn.Conv2d(h_ch, out_ch, ksize, 1, pad)\n",
    "        if self.num_classes > 0:\n",
    "            self.b1 = CategoricalConditionalBatchNorm2d(num_classes, in_ch)\n",
    "            self.b2 = CategoricalConditionalBatchNorm2d(num_classes, h_ch)\n",
    "        else:\n",
    "            self.b1 = nn.BatchNorm2d(in_ch)\n",
    "            self.b2 = nn.BatchNorm2d(h_ch)\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        init.xavier_uniform_(self.c1.weight.data, gain=math.sqrt(2))\n",
    "        init.xavier_uniform_(self.c2.weight.data, gain=math.sqrt(2))\n",
    "        if self.learnable_sc:\n",
    "            init.xavier_uniform_(self.c_sc.weight.data, gain=1)\n",
    "\n",
    "    def forward(self, x, y=None, z=None, **kwargs):\n",
    "        return self.shortcut(x) + self.residual(x, y, z)\n",
    "\n",
    "    def shortcut(self, x, **kwargs):\n",
    "        if self.learnable_sc:\n",
    "            if self.upsample:\n",
    "                h = _upsample(x)\n",
    "            h = self.c_sc(h)\n",
    "            return h\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def residual(self, x, y=None, z=None, **kwargs):\n",
    "        if y is not None:\n",
    "            h = self.b1(x, y, **kwargs)\n",
    "        else:\n",
    "            h = self.b1(x)\n",
    "        h = self.activation(h)\n",
    "        if self.upsample:\n",
    "            h = _upsample(h)\n",
    "        h = self.c1(h)\n",
    "        if y is not None:\n",
    "            h = self.b2(h, y, **kwargs)\n",
    "        else:\n",
    "            h = self.b2(h)\n",
    "        return self.c2(self.activation(h))\n",
    "\n",
    "class ResNetGenerator(nn.Module):\n",
    "    \"\"\"Generator generates 32x32.\"\"\"\n",
    "    def __init__(self, num_features=64, dim_z=128, bottom_width=4, num_classes=0,\n",
    "                 activation=F.relu):\n",
    "        super(ResNetGenerator, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.dim_z = dim_z\n",
    "        self.bottom_width = bottom_width\n",
    "        self.activation = activation\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.l1 = nn.Linear(dim_z, 8 * num_features * bottom_width ** 2)\n",
    "\n",
    "        self.block2 = GenResBlock(num_features * 8, num_features * 4,\n",
    "                            activation=activation, upsample=True,\n",
    "                            num_classes=num_classes)\n",
    "        self.block3 = GenResBlock(num_features * 4, num_features * 2,\n",
    "                            activation=activation, upsample=True,\n",
    "                            num_classes=num_classes)\n",
    "        self.block4 = GenResBlock(num_features * 2, num_features,\n",
    "                            activation=activation, upsample=True,\n",
    "                            num_classes=num_classes)\n",
    "        self.b5     = nn.BatchNorm2d(num_features)\n",
    "        self.conv5  = nn.Conv2d(num_features, 3, 1, 1)\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        init.xavier_uniform_(self.l1.weight.data)\n",
    "        init.xavier_uniform_(self.conv5.weight.data)\n",
    "\n",
    "    def forward(self, z, y=None, **kwargs):\n",
    "        h = self.l1(z).view(z.size(0), -1, self.bottom_width, self.bottom_width)\n",
    "        for i in range(2, 5):\n",
    "            h = getattr(self, 'block{}'.format(i))(h, y, **kwargs)\n",
    "        h = self.activation(self.b5(h))\n",
    "        return torch.tanh(self.conv5(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb843b3-e258-442c-9632-b5de578389ac",
   "metadata": {},
   "source": [
    "## 3.2. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5307b102-d1d2-480e-964f-5b168b867790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, ksize=3, pad=1, activation=F.relu):\n",
    "        super(OptimizedBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "        self.c1 = utils.spectral_norm(nn.Conv2d(in_ch, out_ch, ksize, 1, pad))\n",
    "        self.c2 = utils.spectral_norm(nn.Conv2d(out_ch, out_ch, ksize, 1, pad))\n",
    "        self.c_sc = utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 1, 1, 0))\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        init.xavier_uniform_(self.c1.weight.data, math.sqrt(2))\n",
    "        init.xavier_uniform_(self.c2.weight.data, math.sqrt(2))\n",
    "        init.xavier_uniform_(self.c_sc.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x) + self.residual(x)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        return self.c_sc(F.avg_pool2d(x, 2))\n",
    "\n",
    "    def residual(self, x):\n",
    "        h = self.activation(self.c1(x))\n",
    "        return F.avg_pool2d(self.c2(h), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b1629e-d1d7-4157-8696-88525712d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, h_ch=None, ksize=3, pad=1,\n",
    "                 activation=F.relu, downsample=False):\n",
    "        super(DisResBlock, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.learnable_sc = (in_ch != out_ch) or downsample\n",
    "        if h_ch is None:\n",
    "            h_ch = in_ch\n",
    "        else:\n",
    "            h_ch = out_ch\n",
    "\n",
    "        self.c1 = utils.spectral_norm(nn.Conv2d(in_ch, h_ch, ksize, 1, pad))\n",
    "        self.c2 = utils.spectral_norm(nn.Conv2d(h_ch, out_ch, ksize, 1, pad))\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 1, 1, 0))\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        init.xavier_uniform_(self.c1.weight.data, math.sqrt(2))\n",
    "        init.xavier_uniform_(self.c2.weight.data, math.sqrt(2))\n",
    "        if self.learnable_sc:\n",
    "            init.xavier_uniform_(self.c_sc.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x) + self.residual(x)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.learnable_sc:\n",
    "            x = self.c_sc(x)\n",
    "        if self.downsample:\n",
    "            return F.avg_pool2d(x, 2)\n",
    "        return x\n",
    "\n",
    "    def residual(self, x):\n",
    "        h = self.c1(self.activation(x))\n",
    "        h = self.c2(self.activation(h))\n",
    "        if self.downsample:\n",
    "            h = F.avg_pool2d(h, 2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a65ca49-26d0-4483-9f2e-e8c0fd6e792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNResNetProjectionDiscriminator(nn.Module):\n",
    "    def __init__(self, num_features, num_classes=0, activation=F.relu):\n",
    "        super(SNResNetProjectionDiscriminator, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.activation = activation\n",
    "\n",
    "        # First block starts with OptimizedBlock\n",
    "        self.block1 = OptimizedBlock(3, num_features)\n",
    "        # Reduce number of downsampling layers\n",
    "        self.block2 = DisResBlock(num_features, num_features * 2,\n",
    "                            activation=activation, downsample=True)\n",
    "        self.block3 = DisResBlock(num_features * 2, num_features * 4,\n",
    "                            activation=activation, downsample=True)\n",
    "        self.block4 = DisResBlock(num_features * 4, num_features * 8,\n",
    "                            activation=activation, downsample=True)\n",
    "        # Remove block5 and block6 which were causing the size to become too small\n",
    "\n",
    "        # Adjust final linear layer to match new feature size\n",
    "        self.l7 = utils.spectral_norm(nn.Linear(num_features * 8, 1))\n",
    "        if num_classes > 0:\n",
    "            self.l_y = utils.spectral_norm(\n",
    "                nn.Embedding(num_classes, num_features * 8))\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        init.xavier_uniform_(self.l7.weight.data)\n",
    "        optional_l_y = getattr(self, 'l_y', None)\n",
    "        if optional_l_y is not None:\n",
    "            init.xavier_uniform_(optional_l_y.weight.data)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = x\n",
    "        # Reduce number of blocks in forward pass\n",
    "        for i in range(1, 5):  # Changed from range(1, 7)\n",
    "            h = getattr(self, f'block{i}')(h)\n",
    "        h = self.activation(h)\n",
    "        # Global pooling\n",
    "        h = torch.sum(h, dim=(2, 3))\n",
    "        output = self.l7(h)\n",
    "        if y is not None:\n",
    "            output += torch.sum(self.l_y(y) * h, dim=1, keepdim=True)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97c945-7588-4b40-bd0b-18b07bdf4aad",
   "metadata": {},
   "source": [
    "# 4. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e2e6a60-790b-480f-8e8c-94b5ede9cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, data_loader):\n",
    "        # Set device\n",
    "        self.device = torch.device(f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Basic configurations\n",
    "        self.n_classes = 20 \n",
    "        self.data_loader = data_loader\n",
    "\n",
    "        # Model hyperparameters\n",
    "        self.imsize = IMSIZE\n",
    "        self.z_dim = Z_DIM\n",
    "        self.g_conv_dim = G_CONV_DIM\n",
    "        self.d_conv_dim = D_CONV_DIM\n",
    "\n",
    "        # Training settings\n",
    "        self.total_step = TOTAL_STEP\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.g_lr = G_LR\n",
    "        self.d_lr = D_LR\n",
    "        self.beta1 = BETA1\n",
    "        self.beta2 = BETA2\n",
    "        self.DStep = D_STEP\n",
    "        self.GStep = G_STEP\n",
    "\n",
    "        # Paths\n",
    "        self.log_path = os.path.join(LOG_PATH, VERSION)\n",
    "        self.sample_path = os.path.join(SAMPLE_PATH, VERSION)\n",
    "        self.model_save_path = os.path.join(MODEL_SAVE_PATH, VERSION)\n",
    "        self.checkpoint_dir = os.path.join(MODEL_SAVE_PATH, 'checkpoints')\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        # Step settings\n",
    "        self.start_step = 0\n",
    "        self.log_step = LOG_STEP\n",
    "        self.sample_step = SAMPLE_STEP\n",
    "        self.model_save_step = MODEL_SAVE_STEP\n",
    "        self.metric_calculation_step = METRIC_CALCULATION_STEP\n",
    "\n",
    "        # Initialize reporter\n",
    "        self.report_file = os.path.join(LOG_PATH, VERSION, VERSION+\"_report.log\")\n",
    "        self.reporter = Reporter(self.report_file)\n",
    "\n",
    "        # Build model and initialize or load checkpoint\n",
    "        self.build_model()\n",
    "        \n",
    "        # Initialize tensorboard writer\n",
    "        self.writer = SummaryWriter(log_dir=self.log_path)\n",
    "\n",
    "        # Initialize Inception network\n",
    "        self.inception_net = load_inception_net()\n",
    "        \n",
    "        # Get real data statistics for FID\n",
    "        self.real_pool, self.real_logits, self.real_labels = self.get_data_statistics()\n",
    "        self.real_mu = np.mean(self.real_pool, axis=0)\n",
    "        self.real_sigma = np.cov(self.real_pool, rowvar=False)\n",
    "\n",
    "        # Load checkpoint if resuming\n",
    "        if RESUME_TRAINING:\n",
    "            self.load_checkpoint(RESUME_CHECKPOINT)\n",
    "\n",
    "        # Write model\n",
    "        self.reporter.writeModel(self.G.__str__())\n",
    "        self.reporter.writeModel(self.D.__str__())\n",
    "\n",
    "    def save_checkpoint(self, step, is_best=False):\n",
    "        \"\"\"Save complete training state\"\"\"\n",
    "        checkpoint = {\n",
    "            'step': step,\n",
    "            'G_state_dict': self.G.state_dict(),\n",
    "            'D_state_dict': self.D.state_dict(),\n",
    "            'g_optimizer_state_dict': self.g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': self.d_optimizer.state_dict(),\n",
    "            'g_scheduler_state_dict': self.g_scheduler.state_dict() if self.g_scheduler else None,\n",
    "            'd_scheduler_state_dict': self.d_scheduler.state_dict() if self.d_scheduler else None,\n",
    "            'random_state': {\n",
    "                'python': random.getstate(),\n",
    "                'numpy': np.random.get_state(),\n",
    "                'torch': torch.get_rng_state(),\n",
    "                'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None\n",
    "            },\n",
    "            'metrics': {\n",
    "                'best_fid': getattr(self, 'best_fid', float('inf')),\n",
    "                'best_is': getattr(self, 'best_is', 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_{step}.pt')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "            torch.save(checkpoint, best_path)\n",
    "        \n",
    "        # Save latest checkpoint reference\n",
    "        latest_path = os.path.join(self.checkpoint_dir, 'latest.txt')\n",
    "        with open(latest_path, 'w') as f:\n",
    "            f.write(str(step))\n",
    "            \n",
    "        self.reporter.writeInfo(f\"Saved checkpoint at step {step}\")\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path=None):\n",
    "        \"\"\"Load training state from checkpoint\"\"\"\n",
    "        if checkpoint_path is None:\n",
    "            # Try to load latest checkpoint\n",
    "            latest_path = os.path.join(self.checkpoint_dir, 'latest.txt')\n",
    "            if not os.path.exists(latest_path):\n",
    "                self.reporter.writeInfo(\"No checkpoint found, starting from scratch\")\n",
    "                return False\n",
    "                    \n",
    "            with open(latest_path, 'r') as f:\n",
    "                step = int(f.read().strip())\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, f'checkpoint_{step}.pt')\n",
    "    \n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"No checkpoint found at {checkpoint_path}\")\n",
    "    \n",
    "        self.reporter.writeInfo(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        # Restore model and optimizer states\n",
    "        self.G.load_state_dict(checkpoint['G_state_dict'])\n",
    "        self.D.load_state_dict(checkpoint['D_state_dict'])\n",
    "        self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "        self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "        \n",
    "        # Restore scheduler states if they exist\n",
    "        if checkpoint['g_scheduler_state_dict'] and hasattr(self, 'g_scheduler'):\n",
    "            self.g_scheduler.load_state_dict(checkpoint['g_scheduler_state_dict'])\n",
    "        if checkpoint['d_scheduler_state_dict'] and hasattr(self, 'd_scheduler'):\n",
    "            self.d_scheduler.load_state_dict(checkpoint['d_scheduler_state_dict'])\n",
    "        \n",
    "        # Safely restore random states\n",
    "        try:\n",
    "            # Restore Python's random state\n",
    "            random.setstate(checkpoint['random_state']['python'])\n",
    "            \n",
    "            # Restore NumPy's random state\n",
    "            np.random.set_state(checkpoint['random_state']['numpy'])\n",
    "            \n",
    "            # Restore PyTorch's random state\n",
    "            torch_state = checkpoint['random_state']['torch']\n",
    "            if isinstance(torch_state, torch.Tensor):\n",
    "                if torch_state.dtype != torch.uint8:\n",
    "                    torch_state = torch_state.byte()\n",
    "                torch.set_rng_state(torch_state)\n",
    "            \n",
    "            # Restore CUDA random state if available\n",
    "            if torch.cuda.is_available() and checkpoint['random_state']['cuda'] is not None:\n",
    "                cuda_state = checkpoint['random_state']['cuda']\n",
    "                if isinstance(cuda_state, list):\n",
    "                    cuda_state = [state.byte() if state.dtype != torch.uint8 else state \n",
    "                                for state in cuda_state]\n",
    "                torch.cuda.set_rng_state_all(cuda_state)\n",
    "        except Exception as e:\n",
    "            self.reporter.writeInfo(f\"Warning: Could not restore random states: {str(e)}\")\n",
    "            self.reporter.writeInfo(\"Continuing with current random states\")\n",
    "        \n",
    "        # Restore metrics\n",
    "        self.best_fid = checkpoint['metrics']['best_fid']\n",
    "        self.best_is = checkpoint['metrics']['best_is']\n",
    "        \n",
    "        # Set starting step\n",
    "        self.start_step = checkpoint['step']\n",
    "        \n",
    "        self.reporter.writeInfo(f\"Resumed training from step {self.start_step}\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # Data iterator\n",
    "        data_iter = iter(self.data_loader)\n",
    "        \n",
    "        # Fixed input for debugging\n",
    "        sampleBatch = 10\n",
    "        fixed_z = torch.randn(self.n_classes*sampleBatch, self.z_dim)\n",
    "        fixed_z = fixed_z.to(self.device)\n",
    "        fixed_c = sampleFixedLabels(self.n_classes,sampleBatch,self.device)\n",
    "\n",
    "        runingZ,runingLabel = prepare_z_c(self.batch_size, self.z_dim, self.n_classes, device=self.device)\n",
    "\n",
    "        # Start time\n",
    "        start_time = time.time()\n",
    "        self.reporter.writeInfo(f\"Start/Resume training from step {self.start_step}\")\n",
    "        dstepCounter = 0\n",
    "        gstepCounter = 0\n",
    "\n",
    "        # Time limit in seconds (3 days = 72 hours = 259200 seconds)\n",
    "        time_limit = 259200\n",
    "    \n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        # Create GradScaler for mixed precision training\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        # Main training progress bar\n",
    "        pbar = tqdm(range(self.start_step, self.total_step),\n",
    "                    desc='Training',\n",
    "                    total=self.total_step,\n",
    "                    initial=self.start_step)\n",
    "\n",
    "        # Initialize metrics for progress bar\n",
    "        metrics = {'d_loss_real': 0, 'd_loss_fake': 0, 'g_loss': 0}\n",
    "        \n",
    "        try:\n",
    "            for step in pbar:\n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - start_time\n",
    "                if elapsed_time >= time_limit:\n",
    "                    self.reporter.writeInfo(f\"Training stopped: Time limit (3 days) exceeded\")\n",
    "                    self.save_checkpoint(step, is_best=False)\n",
    "                    break\n",
    "                # ================== Train D ================== #\n",
    "                self.D.train()\n",
    "                self.G.train()\n",
    "\n",
    "                if dstepCounter < self.DStep:\n",
    "                    try:\n",
    "                        realImages, realLabel = next(data_iter)\n",
    "                    except StopIteration:\n",
    "                        data_iter = iter(self.data_loader)\n",
    "                        realImages, realLabel = next(data_iter)\n",
    "\n",
    "                    # Move data to device\n",
    "                    realImages = realImages.to(self.device)\n",
    "                    realLabel = realLabel.to(self.device).long()\n",
    "\n",
    "                    # Train with real images\n",
    "                    with autocast():\n",
    "                        d_out_real = self.D(realImages, realLabel)\n",
    "                        d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()\n",
    "\n",
    "                        # Generate fake images\n",
    "                        runingZ.sample_()\n",
    "                        runingLabel.sample_()\n",
    "                        with torch.no_grad():\n",
    "                            fake_images = self.G(runingZ, runingLabel)\n",
    "                        \n",
    "                        d_out_fake = self.D(fake_images.detach(), runingLabel)\n",
    "                        d_loss_fake = torch.nn.ReLU()(1.0 + d_out_fake).mean()\n",
    "                        \n",
    "                        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "                    # Backward pass\n",
    "                    self.reset_grad()\n",
    "                    scaler.scale(d_loss).backward()\n",
    "                    scaler.step(self.d_optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    dstepCounter += 1\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    metrics['d_loss_real'] = d_loss_real.item()\n",
    "                    metrics['d_loss_fake'] = d_loss_fake.item()\n",
    "\n",
    "                # ================== Train G ================== #\n",
    "                else:\n",
    "                    with autocast():\n",
    "                        runingZ.sample_()\n",
    "                        runingLabel.sample_()\n",
    "                        fake_images = self.G(runingZ, runingLabel)\n",
    "                        g_out_fake = self.D(fake_images, runingLabel)\n",
    "                        g_loss = -g_out_fake.mean()\n",
    "\n",
    "                    # Backward pass\n",
    "                    self.reset_grad()\n",
    "                    scaler.scale(g_loss).backward()\n",
    "                    scaler.step(self.g_optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    gstepCounter += 1\n",
    "                    \n",
    "                    # Update metric\n",
    "                    metrics['g_loss'] = g_loss.item()\n",
    "\n",
    "                # Reset counters if necessary\n",
    "                if gstepCounter == self.GStep:\n",
    "                    dstepCounter = 0\n",
    "                    gstepCounter = 0\n",
    "\n",
    "                # Step schedulers if they exist\n",
    "                if hasattr(self, 'g_scheduler') and self.g_scheduler is not None:\n",
    "                    self.g_scheduler.step()\n",
    "                if hasattr(self, 'd_scheduler') and self.d_scheduler is not None:\n",
    "                    self.d_scheduler.step()\n",
    "                    \n",
    "                # Update progress bar with time information\n",
    "                remaining_time = time_limit - elapsed_time if elapsed_time < time_limit else 0\n",
    "                hours_remaining = remaining_time // 3600\n",
    "                minutes_remaining = (remaining_time % 3600) // 60\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'd_real': f\"{metrics['d_loss_real']:.4f}\",\n",
    "                    'd_fake': f\"{metrics['d_loss_fake']:.4f}\",\n",
    "                    'g_loss': f\"{metrics['g_loss']:.4f}\"\n",
    "                })\n",
    "\n",
    "                # Log to tensorboard\n",
    "                if (step + 1) % self.log_step == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "                    \n",
    "                    self.writer.add_scalar('loss/d_real', metrics['d_loss_real'], step + 1)\n",
    "                    self.writer.add_scalar('loss/d_fake', metrics['d_loss_fake'], step + 1)\n",
    "                    self.writer.add_scalar('loss/g_loss', metrics['g_loss'], step + 1)\n",
    "                    self.writer.add_scalar('time/hours_remaining', hours_remaining, step + 1)\n",
    "\n",
    "                # Save generated samples\n",
    "                if (step + 1) % self.sample_step == 0:\n",
    "                    with torch.no_grad():\n",
    "                        fake_images = self.G(fixed_z, fixed_c)\n",
    "                        save_image(denorm(fake_images.data),\n",
    "                                os.path.join(self.sample_path, f'{step + 1}_fake.png'),\n",
    "                                nrow=self.n_classes)\n",
    "\n",
    "                # Save checkpoint\n",
    "                if (step + 1) % self.model_save_step == 0:\n",
    "                    self.save_checkpoint(step + 1)\n",
    "\n",
    "                # Calculate metrics\n",
    "                if (step + 1) % self.metric_calculation_step == 0:\n",
    "                    with tqdm(total=1, desc=\"Calculating FID and IS\") as metric_pbar:\n",
    "                        fid, inception_score = self.get_inception_metrics()\n",
    "                        \n",
    "                        # Check if this is the best model\n",
    "                        is_best = False\n",
    "                        if not hasattr(self, 'best_fid') or fid < self.best_fid:\n",
    "                            self.best_fid = fid\n",
    "                            is_best = True\n",
    "                            \n",
    "                        if not hasattr(self, 'best_is') or inception_score > self.best_is:\n",
    "                            self.best_is = inception_score\n",
    "                            is_best = True\n",
    "\n",
    "                        if is_best:\n",
    "                            self.save_checkpoint(step + 1, is_best=True)\n",
    "                        \n",
    "                        # Log metrics\n",
    "                        self.writer.add_scalar('metrics/FID', fid, step + 1)\n",
    "                        self.writer.add_scalar('metrics/IS', inception_score, step + 1)\n",
    "                        self.reporter.writeTrainLog(step + 1, \n",
    "                            f\"FID: {fid:.4f}, IS: {inception_score:.4f}\")\n",
    "                        \n",
    "                        metric_pbar.update(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Save checkpoint on error\n",
    "            self.reporter.writeInfo(f\"Training interrupted at step {step}: {str(e)}\")\n",
    "            self.save_checkpoint(step)\n",
    "            raise e\n",
    "\n",
    "        # Calculate and log total training time\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"Total time is {total_time}s\")\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        self.reporter.writeInfo(f\"Total training time: {total_time_str}\")\n",
    "        self.writer.add_text('training/total_time', total_time_str)\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        self.save_checkpoint(self.total_step)\n",
    "        pbar.close()\n",
    "        \n",
    "    def get_data_statistics(self):\n",
    "        \"\"\"실제 데이터의 inception statistics를 계산\"\"\"\n",
    "        pool, logits, labels = [], [], []\n",
    "        device = next(self.inception_net.parameters()).device\n",
    "\n",
    "        for i, (x, y) in enumerate(self.data_loader):\n",
    "            x = x.to(device)\n",
    "            with torch.no_grad():\n",
    "                pool_val, logits_val = self.inception_net(x)\n",
    "                pool += [np.asarray(pool_val.cpu())]\n",
    "                logits += [np.asarray(F.softmax(logits_val, 1).cpu())]\n",
    "                labels += [np.asarray(y.cpu())]\n",
    "\n",
    "        pool, logits, labels = [np.concatenate(item, 0) for item in [pool, logits, labels]]\n",
    "        return pool, logits, labels\n",
    "\n",
    "    def accumulate_inception_activations(self, num_inception_images=50000):\n",
    "        \"\"\"생성된 이미지의 inception statistics를 계산\"\"\"\n",
    "        pool, logits, labels = [], [], []\n",
    "        while (torch.cat(logits, 0).shape[0] if len(logits) else 0) < num_inception_images:\n",
    "            with torch.no_grad():\n",
    "                z_, c_ = prepare_z_c(self.batch_size, self.z_dim, self.n_classes, device=self.device)\n",
    "                z_.sample_()\n",
    "                c_.sample_()\n",
    "                images = self.G(z_, c_)\n",
    "                pool_val, logits_val = self.inception_net(images)\n",
    "                pool += [pool_val]\n",
    "                logits += [F.softmax(logits_val, 1)]\n",
    "                labels += [c_]\n",
    "\n",
    "        return (torch.cat(pool, 0),\n",
    "                torch.cat(logits, 0),\n",
    "                torch.cat(labels, 0))\n",
    "\n",
    "    def calculate_inception_score(self, pred, num_splits=10):\n",
    "        \"\"\"Inception Score 계산\"\"\"\n",
    "        scores = []\n",
    "        for index in range(num_splits):\n",
    "            pred_chunk = pred[index * (pred.shape[0] // num_splits):\n",
    "                            (index + 1) * (pred.shape[0] // num_splits), :]\n",
    "            kl_inception = pred_chunk * (np.log(pred_chunk) -\n",
    "                                       np.log(np.expand_dims(np.mean(pred_chunk, 0), 0)))\n",
    "            kl_inception = np.mean(np.sum(kl_inception, 1))\n",
    "            scores.append(np.exp(kl_inception))\n",
    "        return np.mean(scores), np.std(scores)\n",
    "\n",
    "    def calculate_fid(self, mu1, sigma1, mu2, sigma2):\n",
    "        \"\"\"Fréchet Inception Distance 계산\"\"\"\n",
    "        diff = mu1 - mu2\n",
    "        covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "        return fid\n",
    "\n",
    "    def get_inception_metrics(self):\n",
    "        \"\"\"FID와 Inception score를 계산\"\"\"\n",
    "        # 생성된 이미지의 statistics 수집\n",
    "        g_pool, g_logits, g_labels = self.accumulate_inception_activations(num_inception_images=50000)\n",
    "\n",
    "        # Numpy로 변환\n",
    "        g_pool_np = g_pool.cpu().numpy()\n",
    "        g_logits_np = g_logits.cpu().numpy()\n",
    "\n",
    "        # 생성된 이미지의 통계치 계산\n",
    "        mu = np.mean(g_pool_np, axis=0)\n",
    "        sigma = np.cov(g_pool_np, rowvar=False)\n",
    "\n",
    "        # FID 계산\n",
    "        fid = self.calculate_fid(self.real_mu, self.real_sigma, mu, sigma)\n",
    "\n",
    "        # Inception Score 계산\n",
    "        is_mean, is_std = self.calculate_inception_score(g_logits_np)\n",
    "\n",
    "        return fid, is_mean\n",
    "\n",
    "    def sample_for_inception(self):\n",
    "        \"\"\"Sample function that returns images and labels\"\"\"\n",
    "        z_, c_ = prepare_z_c(self.batch_size, self.z_dim, self.n_classes, device=self.device)\n",
    "        z_.sample_()\n",
    "        c_.sample_()\n",
    "        with torch.no_grad():\n",
    "            imgs = self.G(z_, c_)\n",
    "        return imgs, c_\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Initialize generator and discriminator with optimizers and schedulers\"\"\"\n",
    "        # Initialize generator and discriminator\n",
    "        self.G = ResNetGenerator(self.g_conv_dim, self.z_dim, 4, num_classes=self.n_classes)\n",
    "        self.D = SNResNetProjectionDiscriminator(self.d_conv_dim, self.n_classes)\n",
    "    \n",
    "        # Move models to device\n",
    "        self.G = self.G.to(self.device)\n",
    "        self.D = self.D.to(self.device)\n",
    "    \n",
    "        # Initialize optimizers\n",
    "        self.g_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.G.parameters()),\n",
    "            self.g_lr, \n",
    "            [self.beta1, self.beta2]\n",
    "        )\n",
    "        self.d_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.D.parameters()),\n",
    "            self.d_lr, \n",
    "            [self.beta1, self.beta2]\n",
    "        )\n",
    "    \n",
    "        # Initialize learning rate schedulers\n",
    "        self.g_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.g_optimizer,\n",
    "            lr_lambda=lambda step: max(1 - step / self.total_step, 0)\n",
    "        )\n",
    "        self.d_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            self.d_optimizer,\n",
    "            lr_lambda=lambda step: max(1 - step / self.total_step, 0)\n",
    "        )\n",
    "    \n",
    "        # Enable gradient checkpointing for memory efficiency if available\n",
    "        if hasattr(self.G, 'gradient_checkpointing_enable'):\n",
    "            self.G.gradient_checkpointing_enable()\n",
    "        if hasattr(self.D, 'gradient_checkpointing_enable'):\n",
    "            self.D.gradient_checkpointing_enable()\n",
    "    \n",
    "        # Log model architectures\n",
    "        self.reporter.writeInfo(\"Generator Architecture:\")\n",
    "        self.reporter.writeInfo(str(self.G))\n",
    "        self.reporter.writeInfo(\"Discriminator Architecture:\")\n",
    "        self.reporter.writeInfo(str(self.D))\n",
    "    \n",
    "        # Log number of parameters\n",
    "        g_params = sum(p.numel() for p in self.G.parameters() if p.requires_grad)\n",
    "        d_params = sum(p.numel() for p in self.D.parameters() if p.requires_grad)\n",
    "        self.reporter.writeInfo(f\"Generator parameters: {g_params:,}\")\n",
    "        self.reporter.writeInfo(f\"Discriminator parameters: {d_params:,}\")\n",
    "    \n",
    "        # Initialize weights if not loading from checkpoint\n",
    "        if not hasattr(self, 'start_step') or self.start_step == 0:\n",
    "            self.reporter.writeInfo(\"Initializing model weights from scratch\")\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        self.G.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_G.pth'.format(self.chechpoint_step))))\n",
    "        self.D.load_state_dict(torch.load(os.path.join(\n",
    "            self.model_save_path, '{}_D.pth'.format(self.chechpoint_step))))\n",
    "        print('loaded trained models (step: {})..!'.format(self.chechpoint_step))\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.d_optimizer.zero_grad()\n",
    "        self.g_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a76b45-fdb8-4415-99aa-60b9f0905c08",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122d4b9-0c12-43c8-917b-b4f80c162ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # For fast training\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Create directories\n",
    "    makeFolder(MODEL_SAVE_PATH, VERSION)\n",
    "    makeFolder(SAMPLE_PATH, VERSION)\n",
    "    makeFolder(LOG_PATH, VERSION)\n",
    "\n",
    "    # Data loader\n",
    "    data_loader = get_cifar100_superclass_loader(IMAGE_PATH, IMSIZE, \n",
    "                                                 BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "    trainer = Trainer(data_loader)\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f47861-293d-41ce-acba-b3635c238146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련이 끝나면 총 학습 시간이  출력됩니다. \n",
    "# The total training time will be displayed once the training is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39fbb39-fe2c-4434-ab24-8b20f67fd824",
   "metadata": {},
   "source": [
    "# 6. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15ab17-0e35-4831-806a-653bb8290634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inceptionID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ed5c0-0ed9-4ec1-a956-c34fc8dd690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and get samples\n",
    "def sample_from_model(G, batch_size, z_dim, device):\n",
    "    samples_per_class = batch_size // 20  # 각 클래스당 정확한 샘플 수 계산\n",
    "    z = torch.randn(batch_size, z_dim, device=device)\n",
    "    labels = torch.tensor(np.repeat(np.arange(20), samples_per_class)).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        samples = G(z, labels)\n",
    "    \n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1ef9b-27f8-491c-a365-f535fa92153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Data loading\n",
    "    norm_mean = [0.5, 0.5, 0.5]\n",
    "    norm_std = [0.5, 0.5, 0.5]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.Normalize(norm_mean, norm_std)\n",
    "    ])\n",
    "    \n",
    "    # Load CIFAR100\n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=64,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Load inception network\n",
    "    net = inceptionID.load_inception_net()\n",
    "    \n",
    "    # Get real data statistics\n",
    "    print(\"Calculating real data statistics...\")\n",
    "    pool, logits, labels = inceptionID.get_net_output(\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        net=net\n",
    "    )\n",
    "    mu_data, sigma_data = np.mean(pool, axis=0), np.cov(pool, rowvar=False)\n",
    "    \n",
    "    # Load generator from best_model.pt\n",
    "    print(\"Loading best model...\")\n",
    "    checkpoint = torch.load('models/cifar100_superclass/checkpoints/best_model.pt', map_location=device)\n",
    "    \n",
    "    G = ResNetGenerator(\n",
    "        num_features=128,  # g_conv_dim from your config\n",
    "        dim_z=128,        # z_dim from your config\n",
    "        bottom_width=4,\n",
    "        num_classes=20    # for CIFAR100 superclasses\n",
    "    ).to(device)\n",
    "    \n",
    "    G.load_state_dict(checkpoint['G_state_dict'])\n",
    "    G.eval()\n",
    "    \n",
    "    # Create sampling function\n",
    "    def sample():\n",
    "        return sample_from_model(G, batch_size=400, z_dim=128, device=device)\n",
    "    \n",
    "    # Get generator statistics\n",
    "    print(\"Calculating generator statistics...\")\n",
    "    g_pool, g_logits, g_labels = inceptionID.accumulate_inception_activations(\n",
    "        sample, net, 50000\n",
    "    )\n",
    "    \n",
    "    # Ensure we use exactly 50000 samples\n",
    "    g_pool = g_pool[:50000]\n",
    "    g_logits = g_logits[:50000]\n",
    "    g_labels = g_labels[:50000]\n",
    "    \n",
    "    mu, sigma = np.mean(g_pool.cpu().numpy(), axis=0), np.cov(g_pool.cpu().numpy(), rowvar=False)\n",
    "    \n",
    "    # Calculate FID\n",
    "    fid = inceptionID.calculate_fid(mu_data, sigma_data, mu, sigma)\n",
    "    print(f\"FID Score: {fid:.4f}\")\n",
    "    \n",
    "    # Calculate Inception Score\n",
    "    is_mean, is_std = inceptionID.calculate_inception_score(g_logits.cpu().numpy(), 10)\n",
    "    print(f\"Inception Score: {is_mean:.4f} ± {is_std:.4f}\")\n",
    "    \n",
    "    # Calculate Intra-FID\n",
    "    print(\"Calculating Intra-FID...\")\n",
    "    intra_fids_mean, intra_fids = inceptionID.calculate_intra_fid(\n",
    "        pool, logits, labels,\n",
    "        g_pool, g_logits, g_labels,\n",
    "        chage_superclass=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Mean Intra-FID: {intra_fids_mean:.4f}\")\n",
    "    print(\"\\nIntra-FID scores per superclass:\")\n",
    "    for i, score in enumerate(intra_fids):\n",
    "        print(f\"Superclass {i}: {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd170ed-b6a5-4f06-9138-be240c58b886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
